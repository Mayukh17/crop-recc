{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c834963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 00:09:54,693 - INFO - Loading data from Crop_Recommendation.csv\n",
      "2025-04-13 00:09:54,708 - ERROR - Error loading data: Missing required columns in the dataset\n",
      "2025-04-13 00:09:54,708 - ERROR - An error occurred: Missing required columns in the dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing required columns in the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 273\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 248\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m recommender \u001b[38;5;241m=\u001b[39m CropRecommender()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mrecommender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCrop_Recommendation.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[1;32m    251\u001b[0m X_train_scaled, X_test_scaled, y_train, y_test, _ \u001b[38;5;241m=\u001b[39m recommender\u001b[38;5;241m.\u001b[39mpreprocess_data(X, y)\n",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m, in \u001b[0;36mCropRecommender.load_data\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m required_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mph\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainfall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m required_columns):\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required columns in the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[1;32m     88\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing required columns in the dataset"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Crop Recommendation System\n",
    "=========================\n",
    "\n",
    "This script implements a machine learning model to recommend crops based on soil and environmental parameters.\n",
    "The system uses a Random Forest Classifier to predict the most suitable crop based on various soil and\n",
    "environmental features.\n",
    "\n",
    "Features used for prediction:\n",
    "- N: Nitrogen content in soil\n",
    "- P: Phosphorous content in soil\n",
    "- K: Potassium content in soil\n",
    "- temperature: Temperature in degree Celsius\n",
    "- humidity: Relative humidity in %\n",
    "- ph: pH value of soil\n",
    "- rainfall: Rainfall in mm\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Current Date]\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Tuple, Dict, Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CropRecommender:\n",
    "    \"\"\"\n",
    "    A class to handle crop recommendation using machine learning.\n",
    "    \n",
    "    This class encapsulates all the functionality needed to load data,\n",
    "    preprocess it, train a model, and make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = 'crop_recommendation_model.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the CropRecommender.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to save/load the trained model\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        \n",
    "    def load_data(self, file_path: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Load and prepare the dataset.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file containing crop data\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Features (X) and target variable (y)\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If the data file doesn't exist\n",
    "            ValueError: If the data format is incorrect\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {file_path}\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Validate data structure\n",
    "            required_columns = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'label']\n",
    "            if not all(col in df.columns for col in required_columns):\n",
    "                raise ValueError(\"Missing required columns in the dataset\")\n",
    "            \n",
    "            # Separate features and target\n",
    "            X = df.drop('label', axis=1)\n",
    "            y = df['label']\n",
    "            \n",
    "            logger.info(f\"Loaded {len(df)} samples with {len(X.columns)} features\")\n",
    "            return X, y\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Data file not found: {file_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def preprocess_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, StandardScaler]:\n",
    "        \"\"\"\n",
    "        Preprocess the data by splitting into train/test sets and scaling features.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix\n",
    "            y (pd.Series): Target variable\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - Scaled training features\n",
    "            - Scaled testing features\n",
    "            - Training labels\n",
    "            - Testing labels\n",
    "            - Fitted scaler\n",
    "        \"\"\"\n",
    "        logger.info(\"Preprocessing data...\")\n",
    "        \n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        logger.info(f\"Training set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, self.scaler\n",
    "\n",
    "    def train_model(self, X_train: np.ndarray, y_train: np.ndarray) -> RandomForestClassifier:\n",
    "        \"\"\"\n",
    "        Train the Random Forest model.\n",
    "        \n",
    "        Args:\n",
    "            X_train (np.ndarray): Scaled training features\n",
    "            y_train (np.ndarray): Training labels\n",
    "            \n",
    "        Returns:\n",
    "            RandomForestClassifier: Trained model\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Random Forest model...\")\n",
    "        \n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            min_samples_split=2,\n",
    "            min_samples_leaf=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        logger.info(\"Model training completed\")\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_model(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model's performance.\n",
    "        \n",
    "        Args:\n",
    "            X_test (np.ndarray): Test features\n",
    "            y_test (np.ndarray): Test labels\n",
    "            \n",
    "        Returns:\n",
    "            Dict containing evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "            \n",
    "        logger.info(\"Evaluating model performance...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        logger.info(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "        logger.info(\"\\nClassification Report:\")\n",
    "        logger.info(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report\n",
    "        }\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Save the trained model and scaler.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If model or scaler is not trained\n",
    "        \"\"\"\n",
    "        if self.model is None or self.scaler is None:\n",
    "            raise ValueError(\"Model or scaler not trained yet\")\n",
    "            \n",
    "        logger.info(f\"Saving model to {self.model_path}\")\n",
    "        \n",
    "        with open(self.model_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'model': self.model,\n",
    "                'scaler': self.scaler\n",
    "            }, f)\n",
    "            \n",
    "        logger.info(\"Model saved successfully\")\n",
    "\n",
    "    def predict_crop(self, features: np.ndarray) -> str:\n",
    "        \"\"\"\n",
    "        Make crop predictions for new data.\n",
    "        \n",
    "        Args:\n",
    "            features (np.ndarray): Input features to predict\n",
    "                Shape should be (n_samples, n_features)\n",
    "                Features order: [N, P, K, temperature, humidity, ph, rainfall]\n",
    "            \n",
    "        Returns:\n",
    "            str: Predicted crop\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If model or scaler is not trained\n",
    "        \"\"\"\n",
    "        if self.model is None or self.scaler is None:\n",
    "            raise ValueError(\"Model or scaler not trained yet\")\n",
    "            \n",
    "        # Validate input shape\n",
    "        expected_features = 7\n",
    "        if features.shape[1] != expected_features:\n",
    "            raise ValueError(f\"Expected {expected_features} features, got {features.shape[1]}\")\n",
    "            \n",
    "        # Scale the features\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = self.model.predict(features_scaled)\n",
    "        return prediction[0]\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the crop recommendation system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the recommender\n",
    "        recommender = CropRecommender()\n",
    "        \n",
    "        # Load and prepare data\n",
    "        X, y = recommender.load_data('Crop_Recommendation.csv')\n",
    "        \n",
    "        # Preprocess data\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, _ = recommender.preprocess_data(X, y)\n",
    "        \n",
    "        # Train model\n",
    "        recommender.train_model(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        recommender.evaluate_model(X_test_scaled, y_test)\n",
    "        \n",
    "        # Save model\n",
    "        recommender.save_model()\n",
    "        \n",
    "        # Example prediction\n",
    "        logger.info(\"\\nMaking example prediction...\")\n",
    "        sample_features = np.array([[90, 42, 43, 20.8, 82.0, 6.5, 202.9]])\n",
    "        predicted_crop = recommender.predict_crop(sample_features)\n",
    "        logger.info(f\"Predicted crop for sample data: {predicted_crop}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f10103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
